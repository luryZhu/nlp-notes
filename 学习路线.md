# 参考资料

## 1 推荐课程

加州大学伯克利分校: Stat212b Topics Course on Deep Learning

斯坦福大学 CS230: Deep learning 

[斯坦福大学 CS224n: Natural Language Processing with Deep Learning](https://www.bilibili.com/video/BV1pt411h7aT/)

[李宏毅DL]() | [课程网页](http://speech.ee.ntu.edu.tw/~tlkagk/courses_ML20.html)

[李宏毅NLP](https://www.bilibili.com/video/av94522844/) | [课程网页](http://speech.ee.ntu.edu.tw/~tlkagk/courses_DLHLP20.html)

## 2 推荐书籍

[统计自然语言处理（宗成庆）](./参考书/统计自然语言处理.pdf)

[机器学习（周志华）](./参考书/机器学习.pdf)

[花书（三大宗师）](./参考书/花书.pdf)

[Neural network and deep learning 神经网络和深度学习](./参考书/Neural Network and Deep Learning-ch.pdf)

## 3 语言 框架 工具

### python

tensorflow/pytorch，NLTK， sklearn，numpy，gensim

### 工具

论文在线编辑：  https://www.overleaf.com/

相似论文图生成：  https://www.connectedpapers.com/?tdsourcetag=s_pcqq_aiomsg

### 拓展：

NLP入门实例（Tensorflow）： https://blog.csdn.net/Irving_zhang/article/details/69396923

Pytorch入门实例： https://github.com/yunjey/pytorch-tutorial

NLP入门练习（Pytorch）： https://github.com/Alic-yuan/nlp-beginner-finish

机器学习理论推导（白板系列）：https://www.bilibili.com/video/BV1aE411o7qd?from=search&seid=4648042595868324770

## 4 必看Paper-list

[﻿LONG SHORT-TERM MEMORY](./paper/LSTM.pdf)

﻿Recurrent neural network based language model

Linguistic regularities in continuous space word representations

﻿Distributed Representations of Words and Phrases and their Compositionality

[﻿Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation](./paper/Encoder-Decoder.pdf)

[﻿Sequence to Sequence Learning with Neural Networks](./paper/seq2seq.pdf)

[Neural Machine Translation by Jointly Learning to Align and Translate](./paper/align-translate.pdf)

End-To-End Memory Networks

Effective Approaches to Attention-based Neural Machine Translation

Language Modeling with Gated Convolutional Networks

Convolutional Sequence to Sequence Learning

Neural Machine Translation in Linear Time

A Structured Self-attentive Sentence Embedding

[Attention Is All You Need](./paper/attention.pdf)

Improving Language Understanding by Generative Pre-Training

Deep contextualized word representations

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

ERNIE: Enhanced Language Representation with Informative Entities

# 任务计划

| 类目             | 完成时间 | 数据集                                       | 模型                               | 任务要求                                                     |
| ---------------- | -------- | -------------------------------------------- | ---------------------------------- | ------------------------------------------------------------ |
| 文本相似度匹配   | 2周      |                                              | WMD距离、cos相似度                 | 只使用Numpy完成模型搭建  理解metric：召回率、准确率、F1-score |
| 统计语言模型     | 2周      | Wiki-103                                     | 词袋模型、N-garm模型、word2vec模型 | 只使用Numpy完成模型搭建  理解CBOW/SKIP-gram模型  理解语言模型metric：困惑度 |
| NN语言模型       | 2周      | Wiki-103                                     | RNN模型、CNN模型                   | 使用NN框架完成模型搭建  理解词向量生成的过程                 |
| NN文本分类       | 2周      | Kaagle：Sentiment Analysis on Movie  Reviews | CNN模型                            | 使用NN框架完成模型搭建  实现多视觉CNN模型  熟练使用fasttext  |
| NN文本相似度匹配 | 2周      |                                              | Attention模型                      | 使用NN框架完成模型搭建  实现加性attention和乘性attention2种模型  熟练使用bert |
| NN-NER模型       | 2周      | CoNLL/CNN/Daily  Mail                        | LSTM+CRF                           | 使用NN框架完成模型搭建  理解并实现CRF                        |
| NN-文本生成模型  | 4周      | wmt/CNN/Daily  Mail/LCSTS                    | Transformer/ConS2S                 | 使用encoder-decoder框架完成模型建模  使用NN框架完成模型搭建  |